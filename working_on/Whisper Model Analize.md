之前我一直在用着 FunASR 的 Paraformer 模型，它真的相当方便，不需要脑子的那种。

对于中英文长段落混合，夹杂词语的混合，以及超长音频，不同音频格式都支持得很好。而我需要做的只有 `pip install FunASR`。

而它有个局限就是仅支持中英文。而我大概一年前就计划者要引入日文语种，但是一直停滞，这点现在想起来还是会有些无地自容。

## Q&A

#### Q:它是否只支持 wav？

A:  不，它支持更多，不需要手动转 wav.

使用它需要 `openai-whisper` 和 `ffmpeg`, 一个是 python 依赖，一个是系统依赖。

转 wav 应该是 openai-whisper 自己调用的。

而在我的项目里，对于 ffmpeg 是不需要用户手动安装并且添加到 system_path 的，所以，我需要了解 whisper 是否支持手动传入 FFMPEG_PATH.

> 根据 gemini 解释，它调用的似乎不是 FFMPEG_PATH, 而是使用 ffmpeg-pyton 实现了类似的功能。<br>
> 但是还需要再了解一下如果用户系统路径中没有 ffmpeg 是否可以运行。<br>

#### Q: 它是否需要强指定 ja, en 等参数？

A: 可以指定，但不是推荐的和必要的。

它支持直接选择目标语言，但是可能影响它长段落混合和夹杂词语混合的转录精度。

为什么自动检测是更好的？

比如 FunASR 模型运行通常分 vad（*Voice Activity Detection*） 和 asr（*Automatic Speech Recognition*） 。前者将模型切片(*Segmenting*), 即根据语音活动，切出连续的语言活动片段，比如一句话，而每句话之间的间歇，比如规定长于 500ms（这个只是举例），通常认为这句话结束了。

然后模型会对片段进行 asr, 这步应该是批处理的，它一次会识别几个片段的具体内容（这应该也是长音频不会爆显存的原因）。语言检测应该也是发生在这里，而如果用户给了一个 japanese 的强约束，那么可能会影响这种分步骤的颗粒读和精细度，当一个音频中语言不止一种的时候这种现象会更明显。

而相比于 FunASR 的组合模型， whisper 的模型似乎是混合模型。即一个模型就可以完成以上任务。比如这次我要使用的 whisper-turbo。

