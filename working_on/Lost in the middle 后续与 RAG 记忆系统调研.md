## Lost in the Middle

Lost in the Middle å‘å¸ƒä¹‹åå·²ç»è¿‡äº†å¾ˆä¹…ã€‚

æˆ‘å‰é˜µå­å†™äº†è¿™ä¸ªï¼š

[Lost in the middle - how it affect LLM agent system](https://xnnehang.top/posts/explore/lost-in-the-middle)

é‡Œé¢è¿˜æœ‰ä¸å°‘è¯´æ³•ç»ä¸èµ·æ¨æ•²ï¼Œå› ä¸ºæ¥æºä¸å¤ªå¯é ã€‚

ä½†å¯ä»¥å®šä¸‹æ¥çš„æ˜¯ï¼š

## Lost in the Middle å‰ç½®æ¦‚å¿µ

æˆ‘ä¼šåˆ†æˆç»“è®º+è¯æ®çš„å½¢å¼æ¥å†™ã€‚

- å¤§æ¨¡å‹åœ¨æ¥å—è¾“å…¥æ—¶ä¼šæŠŠå½“å‰ç”¨æˆ·è¾“å…¥åŠ åˆ°æ•´ä¸ªå†å²ä¸Šä¸‹æ–‡ä¸­ç„¶åè¿›è¡Œæ¨ç†ï¼Œè€Œä¸æ˜¯ä»…ä»…å¯¹å½“å‰è¾“å…¥å’Œæé—®è¿›è¡Œæ¨ç†ã€‚æ‰€ä»¥ä¸Šä¸‹æ–‡è¶Šé•¿ï¼Œtoken æ¶ˆè€—è´¹ç”¨è¶Šé«˜ï¼Œä¸¤è€…çº¿æ€§ç›¸å…³ã€‚

æˆ‘è¯´çš„è¿™ä¸ªæƒ…å†µæ˜¯é’ˆå¯¹äº Stateless LLM, Gemini-2.5, OpenAI-5.2 ç­‰ç­‰å¸¸åœ¨ API ä¸­ä½¿ç”¨çš„æ¨¡å‹éƒ½æ˜¯ Stateless LLMã€‚å®ƒçš„ç‰¹å¾æ˜¯æ¯æ¬¡æ–‡æœ¬ç”Ÿæˆè¯·æ±‚æ˜¯ç‹¬ç«‹ä¸”æ— çŠ¶æ€ã€‚æ‰€ä»¥å¦‚æœè¦å®ç°è¿ç»­å¯¹è¯éƒ½è¦æ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªåˆ—è¡¨æŠŠä¹‹å‰çš„å†å²å¯¹è¯åŠ è¿›å»ã€‚ï¼ˆå½“ç„¶å¹¶ä¸ä»£è¡¨æ¯æ¬¡éƒ½è¦ä» 0 è®¡ç®—ï¼Œå…·ä½“å‚è§ä¸‹æ–¹ï¼‰

>While **each text generation request is independent and stateless**, you can still implementÂ **multi-turn conversationsÂ by providing additional messages as parameters to your text generation request**.    [OpenAI Conversation State](https://platform.openai.com/docs/guides/conversation-state)

å½“ç„¶ï¼Œè¿™é‡Œä¹Ÿè®¸åªèƒ½è¯´æ˜ API è°ƒç”¨æ—¶æ¯æ¬¡è¾“å…¥éƒ½éœ€è¦å®Œæ•´è¾“å…¥ï¼Œä¸èƒ½å®Œå…¨è¯´æ˜æ¨¡å‹æ¯æ¬¡æ¨ç†æ—¶ä¸€å®šè¦æ‰€æœ‰çš„è¾“å…¥ã€‚ï¼ˆå› ä¸ºè¿™çœ‹ä¸Šå»æ˜¯å¤ç”¨ç‡å¾ˆä½çš„ä¸€ä¸ªåšæ³•ã€‚ï¼‰

ä½†äº‹å®æ˜¯ï¼Œè¿™æ­£æ˜¯ LLM çš„åº•å±‚è®¡ç®—é€»è¾‘ï¼Œæˆ–è€…è¯´ç”±äºåº•å±‚çš„ Transformerï¼Œæ‰€æœ‰çš„ token $x_1$~$x_n$ ä¼šè¢«ä½œä¸º $x_{n+1}$ ~ $x_m$  çš„è®¡ç®—æ¡ä»¶ã€‚

> The prompt phase takes the whole user prompt (ğ‘¥1, . . . , ğ‘¥ğ‘›) as input and computes the probability of the first new token ğ‘ƒ (ğ‘¥ğ‘›+1 | ğ‘¥1, . . . , ğ‘¥ğ‘›).  <br>
>[Efficient Memory Management for Large Language Model Serving with PagedAttention](https://web.stanford.edu/class/cs240/readings/vllm.pdf)

ä½†å®é™…ä¸Šè™½ç„¶æ¨¡å‹æ¯æ¬¡è¾“å…¥éƒ½æ˜¯æ•´ä¸ªä¸Šä¸‹æ–‡ï¼Œä½†å¹¶ä¸æ˜¯æ¯æ¬¡éƒ½ä» 0 è®¡ç®—ã€‚ KV-cache æ˜¯å¾ˆå¥½çš„ä¾‹å­ï¼Œå®ƒå‡å°‘äº†é‡å¤è®¡ç®—å¸¦æ¥çš„è®¡ç®—é‡ã€‚

> At iteration ğ‘¡, the model takes one token ğ‘¥ğ‘›+ğ‘¡ as input and computes the probability ğ‘ƒ (ğ‘¥ğ‘›+ğ‘¡+1 | ğ‘¥1, . . . , ğ‘¥ğ‘›+ğ‘¡) with the key vectors ğ‘˜1, . . . , ğ‘˜ğ‘›+ğ‘¡ and value vectorsğ‘£1, . . . , ğ‘£ğ‘›+ğ‘¡ . Note that the key and value vectors at positions 1 to ğ‘› + ğ‘¡ âˆ’ 1 are cached at previous iterations, only the new key and value vector ğ‘˜ğ‘›+ğ‘¡ and ğ‘£ğ‘›+ğ‘¡ are computed at this iteration. <br> [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://web.stanford.edu/class/cs240/readings/vllm.pdf)


- è®¡é‡ä¸Šä¸‹æ–‡çª—å£ä¸Šé™çš„å•ä½æ˜¯ tokenï¼Œåˆ¤æ–­æ˜¯å¦è¶…å‡ºä¸Šé™æ˜¯çœ‹å•æ¬¡è¯·æ±‚ä¸­æ¨¡å‹èƒ½å…±åŒç†è§£çš„è¾“å…¥+è¾“å‡º token æ€»é‡ä¸Šé™ï¼Œæ‰€ä»¥ç®€å•æ¥è¯´å¯ä»¥ç»†åˆ†ä¸ºè¾“å…¥è¶…ä¸Šé™ï¼Œå’Œè¾“å‡ºè¶…ä¸Šé™ï¼Œä½†ä¸¤è€…è¡¨ç°å‡ºæ¥çš„ç»“æœæ²¡ä»€ä¹ˆã€‚

ä¸Šä¸‹æ–‡çª—å£ï¼ˆcontext windowï¼‰æ˜¯**å•æ¬¡è¯·æ±‚**ä¸­æ¨¡å‹èƒ½å¤„ç†çš„ **token æ€»é‡ä¸Šé™**ã€‚

å¯¹ OpenAI çš„ä¸€äº›æ¨¡å‹/æ¥å£è¡¨è¿°æ¥è¯´ï¼Œè¿™ä¸ªä¸Šé™æ˜ç¡®åŒ…å«ï¼š**input tokens + output tokens + reasoning tokens**ã€‚

> The context window is the maximum number of tokens that can be used in a single request. <br>
>  This max tokens number includes input, output, and reasoning tokens.<br>
>  [Conversation state](https://platform.openai.com/docs/guides/conversation-state?utm_source=chatgpt.com)


- å¦å¤–æ—©æœŸå¯¹äºå•æ¬¡æç¤ºè¯è¾“å…¥çš„ä¸Šé™ä¸€èˆ¬ä¹Ÿæœ‰ä¸€äº›æš´åŠ›çº¦æŸæ¯”å¦‚å•æ¬¡è¾“å…¥ < 10000å­—ã€‚ä½†å¤§æ¨¡å‹æ˜¯å¹¶æ²¡æœ‰æ¨ç†æ—¶ä¸€èˆ¬å¹¶æ²¡æœ‰è¿™æ–¹é¢çš„ç›´æ¥çº¦æŸã€‚

- è¶…è¿‡ä¸Šä¸‹æ–‡çª—å£æ—¶ï¼Œè¦ä¹ˆè¯·æ±‚å¤±è´¥ï¼ˆAPI/æ¨¡å‹ç›´æ¥æŠ¥é”™ï¼‰ï¼Œè¦ä¹ˆç³»ç»Ÿåªä¿ç•™/å‹ç¼©/å¿½ç•¥ä¸€éƒ¨åˆ†ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´å›å¤ä¸å†è¦†ç›–å…¨éƒ¨å†…å®¹ã€‚

è¿™é‡Œæ˜¯ OpenAI API åœ¨ç¢°åˆ°è¶…å‡ºä¸Šä¸‹æ–‡æ—¶ç»™ç”¨æˆ·çš„`truncation`ç­–ç•¥ï¼Œå¯ä»¥è‡ªè¡Œé€‰æ‹©ã€‚

> `auto`: If the input to this Response exceeds the model's context window size, the model will truncate the response to fit the context window by dropping items from the beginning of the conversation.<br>
> `disabled`Â (default): If the input size will exceed the context window size for a model, the request will fail with a 400 error.<br>
> [OpenAI Responses](https://platform.openai.com/docs/api-reference/responses?utm_source=chatgpt.com)


- è¶Šæ¥è¿‘ä¸Šä¸‹æ–‡ä¸Šé™ï¼Œæ¨¡å‹çš„ U å½¢æ›²çº¿ç‰¹å¾è¶Šæ˜æ˜¾ï¼Œ Lost in the middle è¶Šä¸¥é‡ã€‚

[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172)

ä»¥åŠå…³äº Lost in the Middle æ¦‚å¿µçš„ä¸€äº›è¯¦ç»†å†…å®¹æˆ‘ä¹Ÿæ›¾å†™è¿‡ã€‚

[Lost in the middle - how it affect LLM agent system](https://xnnehang.top/posts/explore/lost-in-the-middle)


## Lost in the Middle æ˜¯å¦ä¾ç„¶å­˜åœ¨äºè¿‘æœŸçš„è¯­è¨€æ¨¡å‹

ä¹‹åæ˜¯ç›®å‰çœŸæ­£å…³å¿ƒçš„ï¼Œæ˜¯ Lost in the Middle çš„ç°è±¡ï¼Œåœ¨æœ€è¿‘çš„ LLM ä¸­æ˜¯å¦è¢«è§£å†³ã€‚

å½“æ—¶æˆ‘çš„æ¨æ–­æ˜¯å¹¶æœªè¢«è§£å†³ï¼Œå› ä¸º LLM çš„åº•å±‚å¹¶æ²¡æœ‰å˜åŒ–ã€‚ä½†ç›®å‰éœ€è¦æ¥æ‰¾äº›è¯æ®ã€‚

å…³äº Lost in the Middle (ä»¥ä¸‹ç®€ç§° LITM)çš„ç°è±¡ï¼Œæˆ‘æ‰¾åˆ°äº†è¿™äº›ç›¸å…³å†…å®¹ï¼š

- [**LongBench v2ï¼š é•¿è¯­å¢ƒã€å¤šä»»åŠ¡ä¸‹æ¨¡å‹çš„æ¨ç†ç†è§£èƒ½åŠ›è¯„åˆ†** ](https://longbench2.github.io/?utm_source=chatgpt.com): Gemini-2.5 Flash, GLM-4.5 ç­‰ç­‰æœ€è¿‘ä¸€å¹´çš„æ— æ€ç»´é“¾æ¨ç†çš„æ¨¡å‹åœ¨ zero-shot without COT (é›¶æ ·æœ¬ï¼Œæ— å¤–æ¥æ•°æ®åº“æˆ–è€…æœç´¢å¼•æ“ï¼›æ— æ€ç»´é“¾)çš„æ–¹å¼ä¸­ä¸Š,ç”± Short -> Long è·³è·ƒæ—¶ï¼Œå‡†ç¡®ç‡æ‰äº†å¾ˆå¤šã€‚

è¯´æ˜é•¿æ–‡æœ¬å¯¹äºç›®å‰æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ä¾ç„¶å½±å“å¾ˆå¤§ï¼Œè™½ç„¶ä¸èƒ½æ–­è¨€è¿™ä¸ªæ—¶ç”±äº LITM çš„ç°è±¡ï¼Œå› ä¸ºè¿™é‡Œä½“ç°çš„æ˜¯é•¿åº¦æ•ˆåº”ã€‚è€Œ LITM åˆ™æ˜¯ä½ç½®æ•ˆåº”ã€‚

å¦å¤–è¿™é‡Œæ¯”è¾ƒå¥½å¥‡çš„ä¸€ç‚¹æ˜¯ï¼Œå…·æœ‰æ€ç»´é“¾(CoT)çš„æ¨¡å‹åœ¨æ–‡æœ¬å˜é•¿æ—¶ï¼Œæœ‰æ—¶æ€§èƒ½ä¸é™åå¢ï¼Œè¿™å¾ˆæœ‰æ„æ€ã€‚

> ç»è¿‡æŸ¥é˜…ï¼Œå¾—çŸ¥ CoT æ¨¡å‹ä¹‹æ‰€ä»¥åœ¨ä»»åŠ¡å‡†ç¡®ç‡ä¸Šæœ‰æé«˜æ˜¯å› ä¸ºæ˜¯ä¼šå‡å°‘â€œçŒœæµ‹å¼â€è¾“å‡ºä¸æ ¼å¼é”™è¯¯ã€‚å¹¶ä¸æ˜¯æ ¹æœ¬ä¸Šè§£å†³äº†é•¿åº¦æ•ˆåº”ï¼ˆæé«˜ç†è§£èƒ½åŠ›ï¼‰ã€‚è¿™å°±å¥½åƒä¸€åœºè€ƒè¯•é‡Œï¼Œ without CoT çš„å­¦ç”Ÿåªæœ‰ 120 åˆ†é’Ÿï¼Œä¸”æ‰€æœ‰é—®é¢˜åªåšäº†ä¸€éå°±äº¤å·ï¼Œä»¥åŠå®ƒè¿˜æœ‰è‡ªå·±çš„è‰ç¨¿çº¸ï¼ˆCoTï¼‰,å¯ä»¥åˆ†æ­¥æ‹†è§£é—®é¢˜ï¼Œè€Œ CoT çš„æ—¶é—´ä¸Šæ›´å……è£•ï¼Œå¯ä»¥åå¤åœ°æ£€æŸ¥è‡ªå·±æ˜¯å¦æœ‰ä¸€äº›ç²—å¿ƒçŠ¯é”™ã€‚


-  [# \inftyBench: Extending Long Context Evaluation Beyond 100K Tokens](https://arxiv.org/abs/2402.13718?utm_source=chatgpt.com): å¾ˆå¤šæ¨¡å‹/ç³»ç»Ÿâ€œèƒ½åƒè¿›å»â€100K/200K tokensï¼ˆæ ‡ç§°çª—å£ï¼‰ï¼Œä½†åœ¨çœŸå®ä»»åŠ¡é‡Œâ€œç”¨å¾—èµ·æ¥â€çš„ä¿¡æ¯é‡ï¼ˆæœ‰æ•ˆä¸Šä¸‹æ–‡ï¼‰æ˜æ˜¾æ›´å°ï¼›å½“é•¿åº¦ç»§ç»­æ‹‰é•¿ï¼Œæ€§èƒ½ä¼šé€€åŒ–ã€‚

ä»–ä»¬æå‡ºäº†ä¸€ä¸ªæœ‰æ•ˆçª—å£çš„æ¦‚å¿µï¼Œæœ‰çš„å¤§æ¨¡å‹å³ä½¿å£°ç§°å¯ä»¥æ¥å— 100/200k  tokens æˆ–è€… 1M tokensï¼Œä½†æ˜¯å®é™…çš„æ¶ˆèå®éªŒä¸­ï¼Œå½“ä¸Šä¸‹æ–‡é•¿åº¦è¶Šé•¿ï¼Œæ€§èƒ½è¡¨ç°è¶Šç³Ÿç³•ã€‚

å¦å¤–åœ¨ 5.2 éƒ¨åˆ†æˆ‘çœ‹åˆ°ä»–ä»¬æåˆ°äº† Lost in the Middleã€‚

![image-20260102161101092](https://cdn.xnnehang.top/MrXnneHang/blog_img/refs/heads/main/BlogHosting/img/25/11/202601021511232.png)

è¿™é‡Œæ˜¯ä»–ä»¬åšçš„ä½ç½®æ•ˆåº”çš„å›¾ï¼š

![image-20260102161154099](https://cdn.xnnehang.top/MrXnneHang/blog_img/refs/heads/main/BlogHosting/img/25/11/202601021511100.png)

è¿™é‡Œä»–ä»¬å‘ç°ï¼Œå¯¹äºä¸åŒäº§å•†çš„å¤§æ¨¡å‹ï¼Œä»–ä»¬å¹¶æ²¡æœ‰åœ¨ä½ç½®æ•ˆåº”ä¸Šå‘ˆç°ä¸€è‡´æ€§ã€‚æœ‰çš„åå‘é¦–å› ï¼Œæœ‰çš„åå‘ä¸­é—´ã€‚

è¿™ç‚¹çœ‹èµ·æ¥ä¼¼ä¹å¦å®šäº†ç›®å‰ Lost in the Middle ç°è±¡çš„å­˜åœ¨ã€‚

ä½†æ˜¯æˆ‘ä¸ªäººæŠ±æŒçš„è§‚ç‚¹æ˜¯ï¼ŒLost in the Middle ä¸­æåˆ°çš„ Transformer é¢„è®­ç»ƒæ—¶å­˜åœ¨çš„é—®é¢˜è¿˜æœªè§£å†³ï¼Œåªæ˜¯åœ¨ä¸åŒçš„åº”å¯¹æ–¹å¼ä¸‹å˜æˆäº†ä¸åŒçš„è¡¨ç°æ–¹å¼ã€‚

å¹¶ä¸”æˆ‘åœ¨ä¹‹åæ‰¾åˆ°äº†å¯ä»¥æ”¯æ’‘æˆ‘è§‚ç‚¹çš„æœ‰æ•ˆå®éªŒï¼Œå³è¿™ä¸ªä½ç½®æ•ˆåº”å¯¼è‡´çš„æ³¨æ„åŠ›ç»“æ„æ€§åå·®ä¾ç„¶å­˜åœ¨ã€‚

- [Attention Sinksï¼šå¼€å¤´ token çš„ç»“æ„æ€§é«˜æ³¨æ„åŠ›ï¼ˆsinkï¼‰](https://arxiv.org/pdf/2410.10781)

>â€œstrong attention scores towards initial tokens as a â€˜sinkâ€™ even if they are not semantically important.â€ [arXiv](https://arxiv.org/abs/2309.17453)Â   <br>
> **ç¿»è¯‘ï¼š** æ¨¡å‹ä¼šå¯¹**æœ€åˆçš„ token**ç»™å‡ºå¾ˆå¼ºçš„æ³¨æ„åŠ›åˆ†æ•°ï¼ŒæŠŠå®ƒä»¬å½“æˆâ€œæ±‡ç‚¹ï¼ˆsinkï¼‰â€ï¼Œ**å³ä½¿è¿™äº› token åœ¨è¯­ä¹‰ä¸Šå¹¶ä¸é‡è¦**ã€‚


>â€œattention sinks exist universally in LMs â€¦ even in small models.â€ [arXiv](https://arxiv.org/abs/2410.10781)Â  <br> 
> **ç¿»è¯‘ï¼š** æ³¨æ„åŠ›æ±‡ç‚¹åœ¨å„ç§è¯­è¨€æ¨¡å‹ä¸­**æ™®éå­˜åœ¨**ï¼Œç”šè‡³åœ¨å°æ¨¡å‹é‡Œä¹Ÿå­˜åœ¨ã€‚


>â€œattention sink is observed to emerge during the LM pre-training.â€ [arXiv](https://arxiv.org/abs/2410.10781)Â   <br>
> **ç¿»è¯‘ï¼š** æ³¨æ„åŠ›æ±‡ç‚¹è¢«è§‚å¯Ÿåˆ°ä¼šåœ¨**è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°**ã€‚


è¿™é‡Œå¯¹æ¨¡å‹çš„æ³¨æ„åŠ›ç»“æ„æ€§åå·®è¿›è¡Œäº†ä¸€äº›è¿½è¸ªå’Œç ”ç©¶ã€‚å¹¶ä¸”å‘ç°äº†å®ƒåœ¨å„ç§è¯­è¨€æ¨¡å‹ä¸­æ™®éå­˜åœ¨ï¼Œä¸”å‡ºç°åœ¨é¢„è®­ç»ƒçš„è¿‡ç¨‹ä¸­ã€‚

è¿™å¯ä»¥è¯´æ˜¯ **LITM** çš„ä¸€æ¬¡æº¯æºå’Œè§£é‡Šï¼Œè‡³å°‘ Primacy Effect æ˜¯æ‰¾åˆ°äº†ã€‚


- [Found in the Middle: å³ä½¿æ¨¡å‹è¢«ä¸“é—¨è®­ç»ƒæ¥å¤„ç†é•¿è¾“å…¥ä¸Šä¸‹æ–‡ï¼Œä»ç„¶éš¾ä»¥æ•æ‰ä½äºè¾“å…¥ä¸­é—´çš„ç›¸å…³ä¿¡æ¯ã€‚](https://arxiv.org/abs/2406.16008)

> â€œeven when â€¦ trained â€¦ struggle to capture relevant information â€¦ in the middle.â€ [arXiv](https://arxiv.org/abs/2406.16008)Â    <br>
> å³ä½¿æ¨¡å‹è¢«ä¸“é—¨è®­ç»ƒæ¥å¤„ç†é•¿è¾“å…¥ä¸Šä¸‹æ–‡ï¼Œä»ç„¶éš¾ä»¥æ•æ‰ä½äºè¾“å…¥ä¸­é—´çš„ç›¸å…³ä¿¡æ¯ã€‚


> â€œmitigate this positional bias through a calibration mechanism, found-in-the-middle.â€ [arXiv](https://arxiv.org/abs/2406.16008)Â   <br>
> ä»–ä»¬é€šè¿‡ä¸€ç§æ ¡å‡†æœºåˆ¶ï¼ˆfound-in-the-middleï¼‰æ¥ç¼“è§£è¿™ç§**ä½ç½®åç½®**ã€‚


è¿™ç¯‡è®ºæ–‡å‘ç° LITM å§‹ç»ˆå­˜åœ¨ï¼Œå¹¶ä¸”ç ”ç©¶äº†ä¸€äº›ç¼“è§£ç­–ç•¥ï¼Œå½“ç„¶è¿™ç¼“è§£ç­–ç•¥è‚¯å®šå¹¶ä¸èƒ½æ²»æœ¬ï¼Œæ¯•ç«Ÿé—®é¢˜å‡ºç°åœ¨æ›´æ—©çš„é˜¶æ®µã€‚å½“ç„¶è¿™ä¹Ÿå¯èƒ½æ˜¯ä¸ºä»€ä¹ˆå„å¤§äº§å•†çš„ä½ç½®æ•ˆåº”è¡¨ç°å¾—å„ä¸ç›¸åŒï¼Œç¼“è§£çš„æ–¹å¼ä¸åŒã€‚

å¦‚æœæœ‰å®Œå…¨è§£å†³é¢„è®­ç»ƒé˜¶æ®µçš„æ³¨æ„åŠ›ä½ç½®åå·®çš„æ–¹æ³•ï¼Œé‚£åº”å½“æ˜¯ä¸€ç¯‡é›†ä½“å¼•ç”¨çš„é¢ è¦†æ€§çš„è®ºæ–‡ï¼Œä½†ç›®å‰æ˜¯è¿˜æœªå‡ºç°çš„ã€‚


## è®°å¿†ç³»ç»Ÿ RAG è°ƒç ”

ä¸Šé¢çš„æ‰€æœ‰å·¥ä½œæ€»ç»“èµ·æ¥å°±æ˜¯ï¼š LITM ä¾ç„¶å­˜åœ¨ï¼Œä¸” LITM ç›®å‰åœ¨ä¸åŒäº§å•†å¤§æ¨¡å‹ä¸­ä»¥ä¸åŒçš„æ–¹å¼å­˜åœ¨ï¼Œä¸å†åªæ˜¯å‘ˆç°å‡º U å½¢ã€‚

é‚£ä¹ˆï¼Œæˆ‘ä»¬çš„è®°å¿†ç³»ç»Ÿåº”è¯¥æ€ä¹ˆåšï¼Ÿï¼ˆä½ å¯èƒ½ä¼šè§‰å¾—æœ‰ç‚¹çªå…€ï¼Œä½†æ˜¯æˆ‘æ‰¯é‚£ä¹ˆå¤šç¡®å®æ˜¯ä¸ºäº†æˆ‘çš„è®°å¿†ç³»ç»Ÿã€‚ï¼‰

ç›¸æ¯”äº LITM é‚£è¾¹å­¦æœ¯æ°›å›´ä¼šæ›´é‡ï¼Œ RAG è¿™è¾¹æ›´åå·¥ç¨‹ã€‚æ‰€ä»¥å¯¹äºè®ºæ–‡çš„å¼•ç”¨ä¸ä¼šåƒä¹‹å‰é‚£ä¹ˆå¤šã€‚å› ä¸ºæ¯”èµ·è®ºæ–‡çš„â€œæˆ‘è¿˜æœ‰ä»€ä¹ˆâ€,å·¥ç¨‹è¿™è¾¹æ›´åœ¨åœ¨æ„â€œæˆ‘èƒ½ç”¨æˆ‘ç°åœ¨çš„ä¸œè¥¿åšä»€ä¹ˆâ€ã€‚

è¿™é‡Œæ˜¯æˆ‘çš„ä¸€äº›çµæ„Ÿæ¥æºï¼š

[æµ…è°ˆChatGPTçš„è®°å¿†å®ç°æœºåˆ¶ å…¼è®ºå·¥ç¨‹ç«¯è®°å¿†è®¾è®¡](https://www.lapis.cafe/posts/technicaltutorials/chatgpt-memory-system-breakdown/)

[å…³äºé…’é¦†SillyTavernæ‰€ä»£è¡¨çš„ä¼´ä¾£æ¨¡å‹ç³»ç»Ÿçš„ä¸€äº›å°æ€è€ƒ](https://linux.do/t/topic/781946)

è¿™é‡Œæ˜¯å·²æœ‰çš„å‘è¡¨çš„ä¸€äº›ç ”ç©¶ï¼š

[MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560)

è´ªå¤šåš¼ä¸çƒ‚ï¼Œæˆ‘ä»¬å°±å…ˆä»¥ MemGPT ä¸ºé‡å¿ƒï¼Œå®ƒæå‡ºæ¥çš„ç›¸å½“å¤šè®¾è®¡åœ¨æˆ‘çš„ç³»ç»Ÿä¸­ä¹Ÿä¾ç„¶ä½¿ç”¨ç€ã€‚

### MemGPT

[MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560)

#### 1.MemGPT è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯ä»€ä¹ˆ

LLM å›ºå®šä¸Šä¸‹æ–‡çª—å£å¯¼è‡´ä¸¤ç±»å…¸å‹å¤±è´¥åœºæ™¯ï¼š

1. **é•¿æœŸå¯¹è¯**ï¼šè·¨ä¼šè¯ä¸€è‡´æ€§å·®ã€å¿˜è®°ç”¨æˆ·åå¥½/äº‹å®ã€æ— æ³•å½¢æˆé•¿æœŸå…³ç³»
   
2. **è¶…é•¿æ–‡æ¡£åˆ†æ**ï¼šæ–‡æ¡£è¿œè¶… context windowï¼Œå…³é”®ä¿¡æ¯â€œè¿›ä¸æ¥/ç•™ä¸ä½/ç”¨ä¸ä¸Šâ€
   

MemGPT çš„æ ¸å¿ƒä¸»å¼ æ˜¯ï¼š

> ä¸è¿½æ±‚æŠŠæ‰€æœ‰å†å²éƒ½å¡è¿› promptï¼Œè€Œæ˜¯è®©æ¨¡å‹â€œåƒæ“ä½œç³»ç»Ÿä¸€æ ·â€åœ¨æœ‰é™çª—å£é‡Œ **åŠ¨æ€è°ƒåº¦ä¿¡æ¯**

å¦å¤–ï¼Œè¿™é‡Œå¯ä»¥å›å¿†ä¸‹å‰é¢çš„å†…å®¹ï¼š

- è¶…è¿‡ä¸Šä¸‹æ–‡çª—å£æ—¶ï¼Œè¦ä¹ˆè¯·æ±‚å¤±è´¥ï¼ˆAPI/æ¨¡å‹ç›´æ¥æŠ¥é”™ï¼‰ï¼Œè¦ä¹ˆç³»ç»Ÿåªä¿ç•™/å‹ç¼©/å¿½ç•¥ä¸€éƒ¨åˆ†ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´å›å¤ä¸å†è¦†ç›–å…¨éƒ¨å†…å®¹ã€‚
- [# \inftyBench: Extending Long Context Evaluation Beyond 100K Tokens](https://arxiv.org/abs/2402.13718?utm_source=chatgpt.com): å¾ˆå¤šæ¨¡å‹/ç³»ç»Ÿâ€œèƒ½åƒè¿›å»â€100K/200K tokensï¼ˆæ ‡ç§°çª—å£ï¼‰ï¼Œä½†åœ¨çœŸå®ä»»åŠ¡é‡Œâ€œç”¨å¾—èµ·æ¥â€çš„ä¿¡æ¯é‡ï¼ˆæœ‰æ•ˆä¸Šä¸‹æ–‡ï¼‰æ˜æ˜¾æ›´å°ï¼›å½“é•¿åº¦ç»§ç»­æ‹‰é•¿ï¼Œæ€§èƒ½ä¼šé€€åŒ–ã€‚

æ‰€ä»¥ MemGPT çš„æ ¸å¿ƒé—®é¢˜ç°åœ¨ä¹Ÿä¾ç„¶æ˜¯æ ¸å¿ƒï¼Œå®ƒä»ç„¶å¾ˆå…·å‚è€ƒä»·å€¼ã€‚

#### 2. æ¶æ„æ€»è§ˆï¼šæŠŠ LLM å½“â€œCPUâ€ï¼ŒæŠŠä¸Šä¸‹æ–‡å½“â€œä¸»å­˜â€ï¼ŒæŠŠå¤–ç½®æ•°æ®åº“ä½œä¸ºâ€å¤–å­˜â€œ

![MemGPT çš„ç³»ç»Ÿæ¶æ„å›¾](https://cdn.xnnehang.top/MrXnneHang/blog_img/refs/heads/main/BlogHosting/img/25/11/202601040327459.png)

> å›¾ 3ã€‚ åœ¨ MemGPT ä¸­ï¼Œä¸€ä¸ªå›ºå®šä¸Šä¸‹æ–‡çš„ LLM å¤„ç†å™¨è¢«ä¸€ä¸ªåˆ†å±‚è®°å¿†ç³»ç»Ÿä»¥åŠä¸€ç»„å…è®¸å®ƒç®¡ç†è‡ªèº«è®°å¿†çš„å‡½æ•°æ‰€å¢å¼ºã€‚LLM çš„æç¤ºè¯ tokenï¼ˆè¾“å…¥ï¼‰ï¼Œä¹Ÿå°±æ˜¯ä¸»ä¸Šä¸‹æ–‡ï¼Œç”±ç³»ç»ŸæŒ‡ä»¤ã€å·¥ä½œä¸Šä¸‹æ–‡å’Œä¸€ä¸ª FIFO é˜Ÿåˆ—ç»„æˆã€‚LLM çš„è¡¥å…¨ tokenï¼ˆè¾“å‡ºï¼‰ä¼šè¢«å‡½æ•°æ‰§è¡Œå™¨è§£é‡Šä¸ºå‡½æ•°è°ƒç”¨ã€‚MemGPT ä½¿ç”¨è¿™äº›å‡½æ•°åœ¨ä¸»ä¸Šä¸‹æ–‡ä¸å¤–éƒ¨ä¸Šä¸‹æ–‡ï¼ˆå½’æ¡£å­˜å‚¨ä¸å›å¿†å­˜å‚¨ä¸¤ä¸ªæ•°æ®åº“ï¼‰ä¹‹é—´ç§»åŠ¨æ•°æ®ã€‚LLM è¿˜å¯ä»¥é€šè¿‡åœ¨è¾“å‡ºä¸­ç”Ÿæˆä¸€ä¸ªç‰¹æ®Šçš„å…³é”®å­—å‚æ•°ï¼ˆrequest heartbeat=trueï¼‰æ¥è¯·æ±‚ç«‹å³è¿›è¡Œåç»­çš„ LLM æ¨ç†ï¼Œä»¥ä¾¿æŠŠå¤šä¸ªå‡½æ•°è°ƒç”¨ä¸²è”èµ·æ¥ï¼›è¿™ç§å‡½æ•°é“¾å¼è°ƒç”¨ä½¿ MemGPT èƒ½å¤Ÿæ‰§è¡Œå¤šæ­¥æ£€ç´¢ï¼Œä»è€Œå›ç­”ç”¨æˆ·æŸ¥è¯¢ã€‚<br>

##### ä¸ºä»€ä¹ˆè¦å¯¹ä¸Šä¸‹æ–‡åˆ†åŒºå—

å¯¹äº Prompt Tokens çš„å†…å®¹å¾ˆå¥½ç†è§£ï¼Œå°±æ˜¯æ¨¡å‹çš„ä¸Šä¸‹æ–‡ï¼Œä¸è¿‡è¢«åŒºåˆ†æˆäº†å‡ å—ï¼Œè‡³äºä¸ºä»€ä¹ˆåŒºåˆ†åœ¨ç°åœ¨æ˜¯å¾ˆå¥½ç†è§£çš„ã€‚



æ¯”å¦‚ç”¨ MCP è°ƒç”¨å¤–éƒ¨å·¥å…·çš„æ¨¡å‹ï¼Œå®ƒçš„ä¸Šä¸‹æ–‡ä¸­ä¼šåŒ…å«è¿™æ ·çš„æ­¥éª¤ï¼š

> User: ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ<br>
> Assitant: `[require_search_for_weather]`<br>
> User: Call Tool require_search_for_weather<br>
> Tool: sunny <br>
> User: `[User:ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·,Tool:Sunny]`<br> 
> Assitant: ä»Šå¤©æ˜¯æ™´å¤©ã€‚

åœ¨ç”¨æˆ·é—®â€ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿâ€œå¹¶ä¸”è§¦å‘ MCP çš„æ—¶å€™å®é™…ä¸Šä¸­é—´è¿˜è‡³å°‘åŒ…å«äº†ä¸€ä¸ª Function Call çš„å›å¤ã€æˆ–è€…è¯´å®ƒä¸€ç›´åœ¨ï¼Œåªæ˜¯æœ‰æ—¶ True æœ‰æ—¶ Falseã€‘å’Œ Tool Message çš„å›å¤ï¼Œä»¥åŠæŠŠ Tool Message å’Œç”¨æˆ·é—®é¢˜å†æ¬¡å‘ç»™æ¨¡å‹çš„è¿‡ç¨‹ã€‚è¿™ä¸ªè¿‡ç¨‹æŒ‰ç…§é€»è¾‘ç†è§£ï¼Œåº”è¯¥æ˜¯ system æ¥æä¾›ï¼Œä½†æ˜¯ system çš„ç©ºé—´æ›´é‡‘è´µï¼Œè€Œä¸”å®¹æ˜“ç¨€é‡Šæ³¨æ„åŠ›å’Œå¼•èµ·å†²çªå¹»è§‰ï¼Œæ‰€ä»¥ä¸€èˆ¬æ•´ä¸ªè¿‡ç¨‹éƒ½ä»¥ `role = "user"`  æ¥è¿›è¡Œã€‚

è€Œè¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œä¸å°‘ä¸Šä¸‹æ–‡åœ¨ç»“æœäº§ç”Ÿåéƒ½æ˜¯å¤šä½™çš„ï¼Œæœ€ç»ˆä¸ºäº†ä¿æŒå¹²å‡€å·¥ä½œåŒºå’Œä¸Šä¸‹æ–‡ä¸€èˆ¬éƒ½ä¼šé€‰æ‹©åªä¿ç•™ï¼š

> User: ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ<br>
>  Assitant: ä»Šå¤©æ˜¯æ™´å¤©ã€‚

è¿™ä¹Ÿæ˜¯å·¥ä½œåŒºä¸­ Read-Write çš„æ„ä¹‰æ‰€åœ¨ï¼Œä¸Šä¸‹æ–‡æ˜¯å¯ä»¥ç¼–è¾‘çš„ï¼Œå¯ä»¥æ§åˆ¶æœ‰æ•ˆä¸Šä¸‹æ–‡çš„é•¿åº¦ï¼Œéš”ç¦»ä¸€äº›ç”¨è¿‡ä¸€æ¬¡åå°±ä¸ä¼šå†ç”¨çš„ä¿¡æ¯ï¼Œå¯ä»¥è®©æ¨¡å‹æ€§èƒ½è¡°å‡å˜æ…¢ã€‚

ä¸è¿‡è¿™é‡Œ MemGPT çš„ Function Call åº”è¯¥æ˜¯ Based-Prompt è€Œéæ¨¡å‹åŸç”Ÿæ”¯æŒçš„ï¼ˆå¾®è°ƒå‡ºæ¥çš„ï¼‰ã€‚

è¿™é‡Œåˆ†åŒºå—æŠŠåŸæœ¬çš„ä¸Šä¸‹æ–‡åˆ‡å‰²éš”ç¦»æˆäº†ä¸‰æ®µï¼Œå¯¹äºä¸Šä¸‹æ–‡ç®¡ç†æ¥è¯´æ˜¯å¾ˆå¥½çš„ï¼ŒåŒæ—¶å®ƒè¿˜ä¸ºä¸‰æ®µåˆ†åˆ«è®¾ç½®äº†ä¸åŒçš„å†™å…¥æƒé™å’Œå†™å…¥æ–¹å¼ï¼Œæˆ‘ä¹‹å‰ä¹Ÿæè¿‡ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œä½†æ˜¯æ²¡æœ‰åŒºåˆ†æƒé™æœ‰æ—¶å€™å°±ä¼šå¾ˆéš¾å¤„ç†ã€‚è¿™ç‚¹å¯å‘é¢‡å¤šã€‚

> æˆ‘ä¸ä¼šç…§ç€åŸæ–‡è¯»ï¼Œé‚£çœŸçš„å¾ˆæ— èŠï¼Œæˆ‘ä¼šå¾ˆå¤šåœ°æ–¹è¿›è¡Œç±»æ¯”æ¨è®ºã€‚è€Œä¸”ï¼Œæˆ‘å¯èƒ½ä¸ä¸€å®šä¼šéªŒè¯ï¼Œå¦‚æœæˆ‘èƒ½è¯´æœè‡ªå·±ï¼Œå¯èƒ½ä¼šè®©æ¨¡å‹éªŒè¯ä¸€ä¸‹ã€‚


#####  MemGPT å·¥ä½œåŒºä¸­å„ä¸ªåŒºå—çš„å…·ä½“åŠŸèƒ½

é¦–å…ˆè¿™äº›åŒºå—å…±åŒç»„æˆäº†æˆ‘ä»¬æ‰€è®¤è¯†é‚£ä¸ªä¸Šä¸‹æ–‡ï¼Œå®ƒåªæ˜¯åšäº†**ä¸åŒæƒé™å’Œæ–¹å¼**çš„å†™å…¥è¯»å–æ–¹å¼ï¼Œè¿™ç‚¹å¾ˆæœ‰æ„æ€ã€‚

**System Instructions**

ä¸å¯ä¿®æ”¹ã€‚

æˆ‘æœ€æ—©ä»¥ä¸ºè¿™åº”è¯¥æ˜¯ç³»ç»Ÿæç¤ºè¯ï¼Œäººè®¾ä¹‹ç±»çš„ï¼Œä½†å¹¶ä¸å…¨æ˜¯è¿™æ ·ã€‚

å®ƒçš„è¿™ä¸ª "System" çš„æ„æ€æ˜¯æ“ä½œç³»ç»Ÿï¼Œå®ƒä»¥æç¤ºè¯çš„æ–¹å¼ä¹¦å†™äº†æ§åˆ¶æµï¼Œå„ä¸ªå†…å­˜åŒºå—ç”¨é€”ï¼ŒåŒ…å«å“ªäº›å‡½æ•°ï¼Œå‡½æ•°ç”¨æ³•è¯´æ˜ã€‚è¿™å°±åƒä»€ä¹ˆï¼Œåƒç³»ç»Ÿå†…æ ¸ã€‚å®ƒç»™ LLM æ´—è„‘ï¼Œå‘Šè¯‰å®ƒæ˜¯ä¸€ä¸ªæ“ä½œç³»ç»Ÿï¼Œå…·æœ‰å“ªäº›åŸºç¡€å‘½ä»¤ã€èµ„æºç­‰ç­‰ã€‚è¿™æ˜¯æ‰‹åŠ¨å®ç° Function Call çš„æ ¸å¿ƒï¼Œä½†æ˜¯è¿™ä¸ªæˆåŠŸç‡ä¼°è®¡éå¸¸æ„Ÿäººï¼Œè¿˜å’Œåº•åº§æ¨¡å‹çš„ç†è§£èƒ½åŠ›å¼ºæŒ‚é’©ã€‚

> å„ç§çŒå¥‡äººè®¾è§äº†ä¸å°‘ï¼Œäººæœº play æ˜¯ç¬¬ä¸€æ¬¡è§ã€‚è¿™ä¸ªæ¨¡å‹å…ˆè®°ä½äº†è‡ªå·±æ˜¯ä¸ªæ“ä½œç³»ç»Ÿï¼Œç„¶åæ‰æ˜¯å„ç§äººè®¾ã€‚

**Working Context**

å›ºå®šå¤§å°çš„ä¸€æ®µçª—å£ï¼Œåªèƒ½é€šè¿‡ç‰¹å®šå‡½æ•°æ¥å†™ï¼Œå­˜å…³é”®äº‹å®ã€åå¥½ã€æ™ºèƒ½ä½“äººè®¾ã€‚

> æœ‰ç‚¹åƒä¸€ä¸ªå°å‹çš„æ•°æ®åº“ï¼Œä¸è¿‡ä¸€ç›´åœ¨è€Œä¸æ˜¯ç­‰å¾…æ£€ç´¢ï¼Œå­˜æ”¾ä¸€äº›å…³é”®ä¿¡æ¯ã€‚

**FIFO queue**

æ»šåŠ¨æ¶ˆæ¯å†å²ã€‚

è¿™ä¸€å—æ‰æ˜¯æˆ‘ä»¬æ­£å¸¸ä½¿ç”¨ä¸Šä¸‹æ–‡çš„æ–¹å¼ï¼ŒæŠŠå¯¹è¯ä¸€æ¡ä¸€æ¡æŒ‰é¡ºåºå­˜å…¥ï¼Œæ»šåŠ¨ã€‚

å®ƒåŠ äº†ä»€ä¹ˆï¼š

è¿™æ˜¯ä¸€ä¸ªé˜Ÿåˆ—ï¼Œé•¿åº¦ä¹Ÿæ˜¯å›ºå®šï¼Œå½“ä¸´è¿‘é˜Ÿæ»¡çš„æ—¶å€™ä¼šå¼€å§‹å†™å…¥å¤–éƒ¨çš„æ•°æ®åº“ï¼ˆRecallã€Archival Storageï¼Œæ¨¡å‹è‡ªè¡Œå†³ç­–ï¼‰ã€‚

é˜Ÿåˆ—é¦–éƒ¨è¿˜æœ‰ä¸€ä¸ªé€’å½’æ¦‚è¦ï¼Œæ¦‚æ‹¬è¢«é€å‡ºé˜Ÿåˆ—çš„æ›´æ—©çš„æ¶ˆæ¯ã€‚

